<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Package RL &#8212; IBOAT RL 1.0.0 documentation</title>
    
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Package Realistic Simulator" href="package1bis.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="package-rl">
<h1>Package RL<a class="headerlink" href="#package-rl" title="Permalink to this headline">¶</a></h1>
<p>This package contains three classes of RL agent :</p>
<ul class="simple">
<li><a class="reference internal" href="#module-policyLearning" title="policyLearning"><code class="xref py py-class docutils literal"><span class="pre">policyLearning</span></code></a> - That trains a neural network to predict the Q-values of a discrete actions of a policy of the boat behavior.</li>
<li><a class="reference internal" href="#module-dqn" title="dqn"><code class="xref py py-class docutils literal"><span class="pre">dqn</span></code></a> - That trains a neural network to find the optimal policy to avoid stall with discrete actions.</li>
<li><a class="reference internal" href="#module-DDPG" title="DDPG"><code class="xref py py-class docutils literal"><span class="pre">DDPG</span></code></a> - That trains a neural network to find the optimal policy to avoid stall with a continuous set of actions.</li>
</ul>
<p>You&#8217;ll find a tutorial in order to understand how to generate a training scenario at the bottom of the page.</p>
<div class="section" id="module-policyLearning">
<span id="policy-learner"></span><h2>Policy Learner<a class="headerlink" href="#module-policyLearning" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="policyLearning.PolicyLearner">
<em class="property">class </em><code class="descclassname">policyLearning.</code><code class="descname">PolicyLearner</code><span class="sig-paren">(</span><em>state_size</em>, <em>action_size</em>, <em>batch_size</em><span class="sig-paren">)</span><a class="headerlink" href="#policyLearning.PolicyLearner" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>The aim of this class is to learn the Q-value of the action defined by a policy.</p>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">Please note that the policy to learn has to be defined in the methods <a class="reference internal" href="#policyLearning.PolicyLearner.actUnderPolicy" title="policyLearning.PolicyLearner.actUnderPolicy"><code class="xref py py-meth docutils literal"><span class="pre">actUnderPolicy()</span></code></a> and <a class="reference internal" href="#policyLearning.PolicyLearner.actDeterministicallyUnderPolicy" title="policyLearning.PolicyLearner.actDeterministicallyUnderPolicy"><code class="xref py py-meth docutils literal"><span class="pre">actDeterministicallyUnderPolicy()</span></code></a>.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>state_size</strong> (<em>int</em>) &#8211; shape of the input (for convolutionnal layers).</li>
<li><strong>action_size</strong> (<em>int</em>) &#8211; number of action output by the network.</li>
<li><strong>memory</strong> (<em>deque</em>) &#8211; last-in first-out list of the batch.</li>
<li><strong>gamma</strong> (<em>float</em>) &#8211; discount factor.</li>
<li><strong>epsilon</strong> (<em>float</em>) &#8211; exploration rate.</li>
<li><strong>epsilon_min</strong> (<em>float</em>) &#8211; smallest exploration rate that we want to converge to.</li>
<li><strong>epsilon_decay</strong> (<em>float</em>) &#8211; decay factor that we apply after each replay.</li>
<li><strong>learning_rate</strong> (<em>float</em>) &#8211; the learning rate of the NN.</li>
<li><strong>model</strong> (<em>keras.model</em>) &#8211; NN, i.e the model containing the weight of the value estimator.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="policyLearning.PolicyLearner.act">
<code class="descname">act</code><span class="sig-paren">(</span><em>state</em><span class="sig-paren">)</span><a class="headerlink" href="#policyLearning.PolicyLearner.act" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the action that yields the maximum Q-value.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>state</strong> &#8211; state in which we want to chose an action.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">the greedy action.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="policyLearning.PolicyLearner.actDeterministicallyUnderPolicy">
<code class="descname">actDeterministicallyUnderPolicy</code><span class="sig-paren">(</span><em>state</em><span class="sig-paren">)</span><a class="headerlink" href="#policyLearning.PolicyLearner.actDeterministicallyUnderPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p>Policy that reattaches when the angle of attack goes higher than 16 degree</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>state</strong> (<em>np.array</em>) &#8211; state for which we want to know the policy action.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">the policy action.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="policyLearning.PolicyLearner.actRandomly">
<code class="descname">actRandomly</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#policyLearning.PolicyLearner.actRandomly" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="policyLearning.PolicyLearner.actUnderPolicy">
<code class="descname">actUnderPolicy</code><span class="sig-paren">(</span><em>state</em><span class="sig-paren">)</span><a class="headerlink" href="#policyLearning.PolicyLearner.actUnderPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p>Does the same as <a class="reference internal" href="#policyLearning.PolicyLearner.actDeterministicallyUnderPolicy" title="policyLearning.PolicyLearner.actDeterministicallyUnderPolicy"><code class="xref py py-meth docutils literal"><span class="pre">actDeterministicallyUnderPolicy()</span></code></a> instead that the returned action
is sometime taken randomly.</p>
</dd></dl>

<dl class="method">
<dt id="policyLearning.PolicyLearner.evaluate">
<code class="descname">evaluate</code><span class="sig-paren">(</span><em>state</em><span class="sig-paren">)</span><a class="headerlink" href="#policyLearning.PolicyLearner.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate the Q-value of the two actions in a given state using the neural network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>state</strong> (<em>np.array</em>) &#8211; state that we want to evaluate.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">The actions values as a vector.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="policyLearning.PolicyLearner.evaluateNextAction">
<code class="descname">evaluateNextAction</code><span class="sig-paren">(</span><em>stall</em><span class="sig-paren">)</span><a class="headerlink" href="#policyLearning.PolicyLearner.evaluateNextAction" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate the next action without updating the stall state in order to use it during the experience replay
:param np.array state: state for which we want to know the policy action.
:return: the policy action.</p>
</dd></dl>

<dl class="method">
<dt id="policyLearning.PolicyLearner.get_stall">
<code class="descname">get_stall</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#policyLearning.PolicyLearner.get_stall" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="policyLearning.PolicyLearner.init_stall">
<code class="descname">init_stall</code><span class="sig-paren">(</span><em>mean</em>, <em>mdp</em><span class="sig-paren">)</span><a class="headerlink" href="#policyLearning.PolicyLearner.init_stall" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>mean</strong> &#8211; </li>
<li><strong>mdp</strong> &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="policyLearning.PolicyLearner.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#policyLearning.PolicyLearner.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the weight of the network saved in the file into :ivar model
:param name: name of the file containing the weights to load</p>
</dd></dl>

<dl class="method">
<dt id="policyLearning.PolicyLearner.remember">
<code class="descname">remember</code><span class="sig-paren">(</span><em>state</em>, <em>action</em>, <em>reward</em>, <em>next_state</em>, <em>stall</em><span class="sig-paren">)</span><a class="headerlink" href="#policyLearning.PolicyLearner.remember" title="Permalink to this definition">¶</a></dt>
<dd><p>Remember a transition defined by an action <cite>action</cite> taken from a state <cite>state</cite> yielding a transition to a next
state <cite>next_state</cite> and a reward <cite>reward</cite>. [s, a ,r, s&#8217;]</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>state</strong> (<em>np.array</em>) &#8211; initial state (s).</li>
<li><strong>action</strong> (<em>int</em>) &#8211; action (a).</li>
<li><strong>reward</strong> (<em>float</em>) &#8211; reward received from transition (r).</li>
<li><strong>next_state</strong> (<em>np.array</em>) &#8211; final state (s&#8217;).</li>
<li><strong>stall</strong> (<em>int</em>) &#8211; flow state in the final state (s&#8217;).</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="policyLearning.PolicyLearner.replay">
<code class="descname">replay</code><span class="sig-paren">(</span><em>batch_size</em><span class="sig-paren">)</span><a class="headerlink" href="#policyLearning.PolicyLearner.replay" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform the learning on a the experience replay memory.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>batch_size</strong> &#8211; number of samples used in the experience replay memory for the fit.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">the average loss over the replay batch.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="policyLearning.PolicyLearner.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#policyLearning.PolicyLearner.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the weights of the newtork
:param name: Name of the file where the weights are saved</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-dqn">
<span id="dqn"></span><h2>DQN<a class="headerlink" href="#module-dqn" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="dqn.DQNAgent">
<em class="property">class </em><code class="descclassname">dqn.</code><code class="descname">DQNAgent</code><span class="sig-paren">(</span><em>state_size</em>, <em>action_size</em><span class="sig-paren">)</span><a class="headerlink" href="#dqn.DQNAgent" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>DQN agent</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>state_size</strong> (<em>np.shape</em><em>(</em><em>)</em>) &#8211; shape of the input.</li>
<li><strong>action_size</strong> (<em>int</em>) &#8211; number of actions.</li>
<li><strong>memory</strong> (<em>deque</em><em>(</em><em>)</em>) &#8211; memory as a list.</li>
<li><strong>gamma</strong> (<em>float</em>) &#8211; Discount rate.</li>
<li><strong>epsilon</strong> (<em>float</em>) &#8211; exploration rate.</li>
<li><strong>epsilon_min</strong> (<em>float</em>) &#8211; minimum exploration rate.</li>
<li><strong>epsilon_decay</strong> (<em>float</em>) &#8211; decay of the exploration rate.</li>
<li><strong>learning_rate</strong> (<em>float</em>) &#8211; initial learning rate for the gradient descent</li>
<li><strong>model</strong> (<em>keras.model</em>) &#8211; neural network model</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="dqn.DQNAgent.act">
<code class="descname">act</code><span class="sig-paren">(</span><em>state</em><span class="sig-paren">)</span><a class="headerlink" href="#dqn.DQNAgent.act" title="Permalink to this definition">¶</a></dt>
<dd><p>Act ε-greedy with respect to the actual Q-value output by the network.
:param state: State from which we want to use the network to compute the action to take.
:return: a random action with probability ε or the greedy action with probability 1-ε.</p>
</dd></dl>

<dl class="method">
<dt id="dqn.DQNAgent.actDeterministically">
<code class="descname">actDeterministically</code><span class="sig-paren">(</span><em>state</em><span class="sig-paren">)</span><a class="headerlink" href="#dqn.DQNAgent.actDeterministically" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts the action with the highest q-value at a given state.
:param state: state from which we want to know the action to make.
:return:</p>
</dd></dl>

<dl class="method">
<dt id="dqn.DQNAgent.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#dqn.DQNAgent.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the weights for a defined architecture.
:param name: Name of the source file.</p>
</dd></dl>

<dl class="method">
<dt id="dqn.DQNAgent.loadModel">
<code class="descname">loadModel</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#dqn.DQNAgent.loadModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the an architecture from source file.
:param name: Name of the source file.
:return:</p>
</dd></dl>

<dl class="method">
<dt id="dqn.DQNAgent.remember">
<code class="descname">remember</code><span class="sig-paren">(</span><em>state</em>, <em>action</em>, <em>reward</em>, <em>next_state</em><span class="sig-paren">)</span><a class="headerlink" href="#dqn.DQNAgent.remember" title="Permalink to this definition">¶</a></dt>
<dd><p>Remember a transition defined by an action <cite>action</cite> taken from a state <cite>state</cite> yielding a transition to a next
state <cite>next_state</cite> and a reward <cite>reward</cite>. [s, a ,r, s&#8217;]</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>state</strong> (<em>np.array</em>) &#8211; initial state (s).</li>
<li><strong>action</strong> (<em>int</em>) &#8211; action (a).</li>
<li><strong>reward</strong> (<em>float</em>) &#8211; reward received from transition (r).</li>
<li><strong>next_state</strong> (<em>np.array</em>) &#8211; final state (s&#8217;).</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="dqn.DQNAgent.replay">
<code class="descname">replay</code><span class="sig-paren">(</span><em>batch_size</em><span class="sig-paren">)</span><a class="headerlink" href="#dqn.DQNAgent.replay" title="Permalink to this definition">¶</a></dt>
<dd><p>Core of the algorithm Q update according to the current weight of the network.
:param int batch_size: Batch size for the batch gradient descent.
:return: the loss after the batch gradient descent.</p>
</dd></dl>

<dl class="method">
<dt id="dqn.DQNAgent.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#dqn.DQNAgent.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the weights for a defined architecture.
:param name: Name of the output file</p>
</dd></dl>

<dl class="method">
<dt id="dqn.DQNAgent.saveModel">
<code class="descname">saveModel</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#dqn.DQNAgent.saveModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the model&#8217;s weight and architecture.
:param name: Name of the output file</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-DDPG">
<span id="ddpg"></span><h2>DDPG<a class="headerlink" href="#module-DDPG" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="DDPG.DDPGAgent">
<em class="property">class </em><code class="descclassname">DDPG.</code><code class="descname">DDPGAgent</code><span class="sig-paren">(</span><em>state_size</em>, <em>action_size</em>, <em>lower_bound</em>, <em>upper_bound</em>, <em>sess</em><span class="sig-paren">)</span><a class="headerlink" href="#DDPG.DDPGAgent" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>The aim of this class is to learn an optimal policy via an actor-critic structure with 2 separated Convolutional Neural Networks.
It uses the Deep Deterministic Policy Gradient to update the actor network.
This model deals with a continuous space of actions on the rudder, chosen between lower_bound and upper_bound</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>state_size</strong> (<em>int</em>) &#8211; length of the state input (for convolutionnal layers).</li>
<li><strong>action_size</strong> (<em>int</em>) &#8211; number of continuous action output by the network.</li>
<li><strong>lower_bound</strong> (<em>float</em>) &#8211; minimum value for rudder action.</li>
<li><strong>upper_bound</strong> (<em>float</em>) &#8211; maximum value for rudder action.</li>
<li><strong>sess</strong> (<em>tensorflow.session</em>) &#8211; initialized tensorflow session within which the agent will be trained.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>memory</strong> (<em>deque</em>) &#8211; last-in first-out list of the batch buffer.</li>
<li><strong>gamma</strong> (<em>float</em>) &#8211; discount factor.</li>
<li><strong>epsilon</strong> (<em>float</em>) &#8211; exploration rate.</li>
<li><strong>epsilon_min</strong> (<em>float</em>) &#8211; smallest exploration rate that we want to converge to.</li>
<li><strong>epsilon_decay</strong> (<em>float</em>) &#8211; decay factor that we apply after each replay.</li>
<li><strong>actor_learning_rate</strong> (<em>float</em>) &#8211; the learning rate of the NN of actor.</li>
<li><strong>critic_learning_rate</strong> (<em>float</em>) &#8211; the learning rate of the NN of critic.</li>
<li><a class="reference internal" href="#DDPG.DDPGAgent.update_target" title="DDPG.DDPGAgent.update_target"><strong>update_target</strong></a> (<em>float</em>) &#8211; update factor of the Neural Networks for each fit to target</li>
<li><strong>network</strong> (<em>DDPGNetworks.Network</em>) &#8211; tensorflow model which defines actor and critic convolutional neural networks features</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="DDPG.DDPGAgent.act">
<code class="descname">act</code><span class="sig-paren">(</span><em>state</em><span class="sig-paren">)</span><a class="headerlink" href="#DDPG.DDPGAgent.act" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the action given by the Actor network&#8217;s current weights</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>state</strong> &#8211; state in which we want to chose an action.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">the greedy action according to actor network</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="DDPG.DDPGAgent.act_epsilon_greedy">
<code class="descname">act_epsilon_greedy</code><span class="sig-paren">(</span><em>state</em><span class="sig-paren">)</span><a class="headerlink" href="#DDPG.DDPGAgent.act_epsilon_greedy" title="Permalink to this definition">¶</a></dt>
<dd><p>With probability epsilon, returns a random action between bounds
With probability 1 - epsilon, returns the action given by the Actor network&#8217;s current weights</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>state</strong> &#8211; state in which we want to chose an action.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">a random action or the action given by actor</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="DDPG.DDPGAgent.evaluate">
<code class="descname">evaluate</code><span class="sig-paren">(</span><em>state</em>, <em>action</em><span class="sig-paren">)</span><a class="headerlink" href="#DDPG.DDPGAgent.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate the Q-value of a state-action pair  using the critic neural network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>state</strong> (<em>np.array</em>) &#8211; state that we want to evaluate.</li>
<li><strong>action</strong> (<em>float</em>) &#8211; action that we want to evaluate (has to be between permitted bounds)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">The continuous action value.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="DDPG.DDPGAgent.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#DDPG.DDPGAgent.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the weights of the 2 networks saved in the file into :ivar network
:param name: name of the file containing the weights to load</p>
</dd></dl>

<dl class="method">
<dt id="DDPG.DDPGAgent.noise_decay">
<code class="descname">noise_decay</code><span class="sig-paren">(</span><em>e</em><span class="sig-paren">)</span><a class="headerlink" href="#DDPG.DDPGAgent.noise_decay" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies decay on noisy epsilon-greedy actions</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>e</strong> &#8211; current episode playing during learning</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="DDPG.DDPGAgent.remember">
<code class="descname">remember</code><span class="sig-paren">(</span><em>state</em>, <em>action</em>, <em>reward</em>, <em>next_state</em><span class="sig-paren">)</span><a class="headerlink" href="#DDPG.DDPGAgent.remember" title="Permalink to this definition">¶</a></dt>
<dd><p>Remember a transition defined by an action <cite>action</cite> taken from a state <cite>state</cite> yielding a transition to a next
state <cite>next_state</cite> and a reward <cite>reward</cite>. [s, a ,r, s&#8217;]</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>state</strong> (<em>np.array</em>) &#8211; initial state (s).</li>
<li><strong>action</strong> (<em>int</em>) &#8211; action (a).</li>
<li><strong>reward</strong> (<em>float</em>) &#8211; reward received from transition (r).</li>
<li><strong>next_state</strong> (<em>np.array</em>) &#8211; final state (s&#8217;).</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="DDPG.DDPGAgent.replay">
<code class="descname">replay</code><span class="sig-paren">(</span><em>batch_size</em><span class="sig-paren">)</span><a class="headerlink" href="#DDPG.DDPGAgent.replay" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an update of both actor and critic networks on a minibatch chosen among the experience replay memory.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>batch_size</strong> &#8211; number of samples used in the experience replay memory for the fit.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">the average losses for actor and critic over the replay batch.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="DDPG.DDPGAgent.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#DDPG.DDPGAgent.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the weights of both of the networks into a .ckpt tensorflow session file
:param name: Name of the file where the weights are saved</p>
</dd></dl>

<dl class="attribute">
<dt id="DDPG.DDPGAgent.update_target">
<code class="descname">update_target</code><em class="property"> = None</em><a class="headerlink" href="#DDPG.DDPGAgent.update_target" title="Permalink to this definition">¶</a></dt>
<dd><p>Definition of the neural networks</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="tutorial">
<h2>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python"><div class="highlight"><pre><span></span> <span class="n">history_duration</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Duration of state history [s]</span>
 <span class="n">mdp_step</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Step between each state transition [s]</span>
 <span class="n">time_step</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># time step [s] &lt;-&gt; 10Hz frequency of data acquisition</span>
 <span class="n">mdp</span> <span class="o">=</span> <span class="n">MDP</span><span class="p">(</span><span class="n">history_duration</span><span class="p">,</span> <span class="n">mdp_step</span><span class="p">,</span> <span class="n">time_step</span><span class="p">)</span>

 <span class="n">mean</span> <span class="o">=</span> <span class="mi">45</span> <span class="o">*</span> <span class="n">TORAD</span>
 <span class="n">std</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">*</span> <span class="n">TORAD</span>
 <span class="n">wind_samples</span> <span class="o">=</span> <span class="mi">10</span>
 <span class="n">WH</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">mean</span> <span class="o">-</span> <span class="n">std</span><span class="p">,</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">std</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

 <span class="n">hdg0</span><span class="o">=</span><span class="mi">0</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
 <span class="n">mdp</span><span class="o">.</span><span class="n">initializeMDP</span><span class="p">(</span><span class="n">hdg0</span><span class="p">,</span><span class="n">WH</span><span class="p">)</span>

 <span class="n">hdg0_rand_vec</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">18</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">21</span><span class="p">,</span><span class="mi">22</span><span class="p">,</span><span class="mi">24</span><span class="p">)</span>

 <span class="n">action_size</span> <span class="o">=</span> <span class="mi">2</span>
 <span class="n">policy_angle</span> <span class="o">=</span> <span class="mi">18</span>
 <span class="n">agent</span> <span class="o">=</span> <span class="n">PolicyLearner</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">,</span> <span class="n">policy_angle</span><span class="p">)</span>
 <span class="c1">#agent.load(&quot;policy_learning_i18_test_long_history&quot;)</span>
 <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">120</span>

 <span class="n">EPISODES</span> <span class="o">=</span> <span class="mi">500</span>

 <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPISODES</span><span class="p">):</span>
     <span class="n">WH</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">generateWind</span><span class="p">()</span>
     <span class="n">hdg0_rand</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">hdg0_rand_vec</span><span class="p">)</span> <span class="o">*</span> <span class="n">TORAD</span>
     <span class="n">hdg0</span> <span class="o">=</span> <span class="n">hdg0_rand</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

     <span class="n">mdp</span><span class="o">.</span><span class="n">simulator</span><span class="o">.</span><span class="n">hyst</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

     <span class="c1">#  We reinitialize the memory of the flow</span>
     <span class="n">state</span> <span class="o">=</span> <span class="n">mdp</span><span class="o">.</span><span class="n">initializeMDP</span><span class="p">(</span><span class="n">hdg0</span><span class="p">,</span> <span class="n">WH</span><span class="p">)</span>
     <span class="n">loss_sim_list</span> <span class="o">=</span> <span class="p">[]</span>
     <span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">80</span><span class="p">):</span>
         <span class="n">WH</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">generateWind</span><span class="p">()</span>
         <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="hll">         <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">mdp</span><span class="o">.</span><span class="n">transition</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">WH</span><span class="p">)</span>
</span><span class="hll">         <span class="n">agent</span><span class="o">.</span><span class="n">remember</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>  <span class="c1"># store the transition + the state flow in the</span>
</span><span class="hll">         <span class="c1"># final state !!</span>
</span>         <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
         <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">batch_size</span><span class="p">:</span>
<span class="hll">             <span class="n">loss_sim_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">replay</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span>
</span>             <span class="k">print</span><span class="p">(</span><span class="s2">&quot;time: {}, Loss = {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">loss_sim_list</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
             <span class="k">print</span><span class="p">(</span><span class="s2">&quot;i : {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">TORAD</span><span class="p">))</span>
         <span class="c1"># For data visualisation</span>
     <span class="n">loss_over_simulation_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">los</span>    <span class="n">s_sim_list</span><span class="p">])[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">loss_sim_list</span><span class="p">])[</span><span class="mi">0</span><span class="p">])</span>
     <span class="n">loss_of_episode</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_over_simulation_time</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/IBOAT_logo1.png" alt="Logo"/>
            </a></p>
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Package RL</a><ul>
<li><a class="reference internal" href="#module-policyLearning">Policy Learner</a></li>
<li><a class="reference internal" href="#module-dqn">DQN</a></li>
<li><a class="reference internal" href="#module-DDPG">DDPG</a></li>
<li><a class="reference internal" href="#tutorial">Tutorial</a></li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="package1bis.html" title="previous chapter">Package Realistic Simulator</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/package2.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Tristan Karch.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.5.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="_sources/package2.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>