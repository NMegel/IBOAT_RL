%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
 \ifdefined\DeclareUnicodeCharacterAsOptional\else
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
\fi\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}

\usepackage{geometry}
\usepackage{multirow}
\usepackage{eqparbox}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}



\title{IBOAT RL Documentation}
\date{Feb 20, 2018}
\release{1.0.0}
\author{Tristan Karch}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex

\begin{document}

\maketitle
\sphinxtableofcontents
\phantomsection\label{\detokenize{index::doc}}



\chapter{A brief context}
\label{\detokenize{index:a-brief-context}}\label{\detokenize{index:welcome-to-iboat-rl-s-documentation}}
This project presents \sphinxstylestrong{Reinforcement Learning} as a solution to control systems with a \sphinxstylestrong{large hysteresis}. We consider an
autonomous sailing robot (IBOAT) which sails upwind. In this configuration, the wingsail is almost aligned with the upcoming wind. It thus operates like
a classical wing to push the boat forward. If the angle of attack of the wind coming on the wingsail is too great, the flow around the wing detaches leading to
a \sphinxstylestrong{marked decrease of the boat's speed}.

Hysteresis such as stall are hard to model. We therefore proposes an \sphinxstylestrong{end-to-end controller} which learns the stall behavior and
builds a policy that avoids it. Learning is performed on a simplified transition model representing the stochastic environment and the dynamic of the boat.

Learning is performed on two types of simulators, A \sphinxstylestrong{proof of concept} is first carried out on a simplified simulator of the boat coded in Python. The second phase of the project consist of trying to control a \sphinxstylestrong{more realisitic}  model of the boat. For this purpose we use a dynamic library which is derived using the Code Generation tools in Simulink. The executable C are then feeded to Python using the \sphinxquotedblleft{}ctypes\sphinxquotedblright{} library.

On this page, you will find the documentation of the simplified simulator of the boat as well as the documentation of the reinforcement learning tools. Each package contains tutorials to better understand how the code can be used


\chapter{Requirements}
\label{\detokenize{index:requirements}}
The project depends on the following extensions :
\begin{enumerate}
\item {} 
NumPy for the data structures (\sphinxurl{http://www.numpy.org})

\item {} 
Matplotlib for the visualisation (\sphinxurl{https://matplotlib.org})

\item {} 
Keras for the convolutional neural network models (\sphinxurl{https://keras.io})

\end{enumerate}

\scalebox{0.500000}{\sphinxincludegraphics[width=200\sphinxpxdimen,height=70\sphinxpxdimen]{{numpy}.jpeg}} \scalebox{0.500000}{\sphinxincludegraphics[width=200\sphinxpxdimen,height=70\sphinxpxdimen]{{matplotlib}.jpeg}} \scalebox{0.500000}{\sphinxincludegraphics[width=200\sphinxpxdimen,height=60\sphinxpxdimen]{{keras}.png}}


\chapter{Libraries}
\label{\detokenize{index:libraries}}
There are two dynamic libraries available to simulate the realistic model of the boat :
\begin{enumerate}
\item {} 
\sphinxcode{libBoatModel.so} for Linux users

\item {} 
\sphinxcode{libBoatModel.dylib} for Mac users

\end{enumerate}

One has to change the extension of the library in the file \sphinxcode{Simulator\_realistic.py} depending on its OS.
We also provide a guideline to generate such libraries from a simulink (see \sphinxcode{this file} for more information).


\chapter{Contents}
\label{\detokenize{index:contents}}

\section{Package Simulator}
\label{\detokenize{package1:package-simulator}}\label{\detokenize{package1::doc}}
This package contains all the classes required to build a simulation for the learning. In this small paragraph, the physic of the simulator is described
so that the reader can better understand the implementation.

We need the boat to be in a configuration when it sails upwind so that the flow around the sail is attached and the sail works as a wing.
To generate the configuration we first assume that the boat as a target heading \sphinxtitleref{hdg\_target} = 0. The boat as a certain heading \sphinxtitleref{hdg} with respect to the north
and faces an upcoming wind of heading \sphinxtitleref{WH}. To lower the number of parameters at stake we consider that the wind has a \sphinxstylestrong{constant speed} of 15 knts.
The sail is oriented with respect to the boat heading with an angle \sphinxtitleref{sail\_pos} = -40°. The angle of attack of the wind on the sail is therefore equal to
\sphinxtitleref{i = hdg + WH + sail\_pos}. This angle equation can be well understood thanks to the following image.

\noindent{\hspace*{\fill}\scalebox{2.000000}{\sphinxincludegraphics[width=200\sphinxpxdimen,height=180\sphinxpxdimen]{{scheme}.png}}\hspace*{\fill}}

The action taken to change the angle of attack are changes of boat heading \sphinxtitleref{delta\_hdg}. We therefore assume that \sphinxtitleref{sail\_pos} is constant and equal to -40°.
The wind heading is fixed to \sphinxtitleref{WH} = 45°. Finally, there is a delay between the command and the change of heading of \(\tau\) = 0.5 seconds. The simulator can be
represented with the following block diagram. It contains a delay and an hysteresis block that are variables of the simulator class.

\noindent{\hspace*{\fill}\scalebox{2.000000}{\sphinxincludegraphics[width=200\sphinxpxdimen,height=100\sphinxpxdimen]{{block_diagrams}.png}}\hspace*{\fill}}


\subsection{Simulator}
\label{\detokenize{package1:simulator}}\label{\detokenize{package1:module-Simulator}}\index{Simulator (module)}\index{Simulator (class in Simulator)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:Simulator.Simulator}}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{Simulator.}\sphinxbfcode{Simulator}}{\emph{duration}, \emph{time\_step}}{}
Bases: \sphinxcode{object}

Simulator object : It simulates a simplified dynamic of the boat with the config.
\begin{quote}\begin{description}
\item[{Variables}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{time\_step} (\sphinxstyleliteralemphasis{float}) -- time step of the simulator, corresponds to the frequency of data acquisition.

\item {} 
\sphinxstyleliteralstrong{size} (\sphinxstyleliteralemphasis{float}) -- size of the simulation.

\item {} 
\sphinxstyleliteralstrong{delay} (\sphinxstyleliteralemphasis{int}) -- delay between the heading command and its activation.

\item {} 
\sphinxstyleliteralstrong{sail\_pos} (\sphinxstyleliteralemphasis{float}) -- position of the windsail {[}rad{]}.

\item {} 
\sphinxstyleliteralstrong{hdg\_target} (\sphinxstyleliteralemphasis{float}) -- target heading towards which we want the boat to sail.

\item {} 
\sphinxstyleliteralstrong{hdg} (\sphinxstyleliteralemphasis{np.array}\sphinxstyleliteralemphasis{(}\sphinxstyleliteralemphasis{)}) -- array of size \sphinxstylestrong{size} that stores the heading of the boat {[}rad{]}.

\item {} 
\sphinxstyleliteralstrong{vmg} (\sphinxstyleliteralemphasis{np.array}\sphinxstyleliteralemphasis{(}\sphinxstyleliteralemphasis{)}) -- array of size \sphinxstylestrong{size} that stores the velocity made good.

\item {} 
\sphinxstyleliteralstrong{hyst} ({\hyperref[\detokenize{package1:module-Hysteresis}]{\sphinxcrossref{\sphinxstyleliteralemphasis{Hysteresis}}}}) -- Memory of the flow state during the simulations.

\end{itemize}

\item[{Raises}] \leavevmode
\sphinxstyleliteralstrong{ValueError} -- if the size of the simulation is zero or less.

\end{description}\end{quote}
\index{computeNewValues() (Simulator.Simulator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:Simulator.Simulator.computeNewValues}}\pysiglinewithargsret{\sphinxbfcode{computeNewValues}}{\emph{delta\_hdg}, \emph{WH}}{}
Increment the boat headind and compute the corresponding boat velocity. This method uses the hysteresis function
\sphinxcode{Hysteresis.calculateSpeed()}.
to calculate the velocity.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{delta\_hdg} -- increment of heading.

\item {} 
\sphinxstyleliteralstrong{WH} -- Heading of the wind on the wingsail.

\end{itemize}

\item[{Returns}] \leavevmode
the heading and velocities value over the simulated time.

\end{description}\end{quote}

\end{fulllineitems}

\index{getHdg() (Simulator.Simulator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:Simulator.Simulator.getHdg}}\pysiglinewithargsret{\sphinxbfcode{getHdg}}{\emph{k}}{}
\end{fulllineitems}

\index{getLength() (Simulator.Simulator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:Simulator.Simulator.getLength}}\pysiglinewithargsret{\sphinxbfcode{getLength}}{}{}
\end{fulllineitems}

\index{getTimeStep() (Simulator.Simulator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:Simulator.Simulator.getTimeStep}}\pysiglinewithargsret{\sphinxbfcode{getTimeStep}}{}{}
\end{fulllineitems}

\index{incrementDelayHdg() (Simulator.Simulator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:Simulator.Simulator.incrementDelayHdg}}\pysiglinewithargsret{\sphinxbfcode{incrementDelayHdg}}{\emph{k}, \emph{delta\_hdg}}{}
\end{fulllineitems}

\index{incrementHdg() (Simulator.Simulator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:Simulator.Simulator.incrementHdg}}\pysiglinewithargsret{\sphinxbfcode{incrementHdg}}{\emph{k}, \emph{delta\_hdg}}{}
Increment the heading.
:param k: index to increment.
:param delta\_hdg: value of the increment of heading.
:return:

\end{fulllineitems}

\index{plot() (Simulator.Simulator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:Simulator.Simulator.plot}}\pysiglinewithargsret{\sphinxbfcode{plot}}{}{}
\end{fulllineitems}

\index{updateHdg() (Simulator.Simulator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:Simulator.Simulator.updateHdg}}\pysiglinewithargsret{\sphinxbfcode{updateHdg}}{\emph{k}, \emph{inc}}{}
Change the value of the heading at index k.
:param k: index to update.
:param inc:
:return:

\end{fulllineitems}

\index{updateVMG() (Simulator.Simulator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:Simulator.Simulator.updateVMG}}\pysiglinewithargsret{\sphinxbfcode{updateVMG}}{\emph{k}, \emph{vmg}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{k} -- index to update

\item {} 
\sphinxstyleliteralstrong{vmg} -- value of the velocity to update

\end{itemize}

\item[{Returns}] \leavevmode


\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}


\begin{sphinxadmonition}{warning}{Warning:}
Be careful, the delay is expressed has an offset of index. the delay in s is equal to delay*time\_step
\end{sphinxadmonition}


\subsection{Hysteresis}
\label{\detokenize{package1:hysteresis}}\label{\detokenize{package1:module-Hysteresis}}\index{Hysteresis (module)}\index{Hysteresis (class in Hysteresis)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:Hysteresis.Hysteresis}}\pysigline{\sphinxstrong{class }\sphinxcode{Hysteresis.}\sphinxbfcode{Hysteresis}}
Class that remembers the state of the flow and that computes the velocity for a given angle of attack of the wind
on the wingsail.
:ivar float e: state of the flow (0 if attached and 1 if detached)
\index{calculateSpeed() (Hysteresis.Hysteresis method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:Hysteresis.Hysteresis.calculateSpeed}}\pysiglinewithargsret{\sphinxbfcode{calculateSpeed}}{\emph{i}}{}
Calculate the velocity from angle of attack.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{i} (\sphinxstyleliteralemphasis{float}) -- angle of attack

\item[{Returns}] \leavevmode
v - Boat velocity

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{copy() (Hysteresis.Hysteresis method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:Hysteresis.Hysteresis.copy}}\pysiglinewithargsret{\sphinxbfcode{copy}}{}{}~\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
a deepcopy of the object

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (Hysteresis.Hysteresis method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:Hysteresis.Hysteresis.reset}}\pysiglinewithargsret{\sphinxbfcode{reset}}{}{}
Reset the memory of the flow.

\end{fulllineitems}


\end{fulllineitems}



\subsection{Environment}
\label{\detokenize{package1:module-environment}}\label{\detokenize{package1:environment}}\index{environment (module)}\index{wind (class in environment)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:environment.wind}}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{environment.}\sphinxbfcode{wind}}{\emph{mean}, \emph{std}, \emph{samples}}{}
Generetate the wind samples of the environment. The wind intesity is assumed constant and equal to 15 knots
\begin{quote}\begin{description}
\item[{Variables}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{mean} (\sphinxstyleliteralemphasis{float}) -- mean direction of the wind in {[}rad{]}

\item {} 
\sphinxstyleliteralstrong{std} (\sphinxstyleliteralemphasis{float}) -- standard deviation of the wind direction in {[}rad{]}

\item {} 
\sphinxstyleliteralstrong{samples} (\sphinxstyleliteralemphasis{float}) -- number of samples to generate

\end{itemize}

\end{description}\end{quote}
\index{generateGust() (environment.wind method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:environment.wind.generateGust}}\pysiglinewithargsret{\sphinxbfcode{generateGust}}{\emph{Delta\_WH}}{}
Generates a Gust i.e. an important change of wind heading.
:param Delta\_WH: Magnitude of the gust.
:return: The vector of wind heading corresponding to the gust.

\end{fulllineitems}

\index{generateWind() (environment.wind method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:environment.wind.generateWind}}\pysiglinewithargsret{\sphinxbfcode{generateWind}}{}{}
Generates the wind samples
:return: np.array of wind samples

\end{fulllineitems}


\end{fulllineitems}



\subsection{Markov Decision Process (MDP)}
\label{\detokenize{package1:module-mdp}}\label{\detokenize{package1:markov-decision-process-mdp}}\index{mdp (module)}\index{ContinuousMDP (class in mdp)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:mdp.ContinuousMDP}}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{mdp.}\sphinxbfcode{ContinuousMDP}}{\emph{duration\_history}, \emph{duration\_simulation}, \emph{delta\_t}, \emph{LOWER\_BOUND}, \emph{UPPER\_BOUND}}{}
Markov Decision process modelization of the transition
Based on Realistic Simulator of Iboat autonomous sailboat provided by Simulink
Compatible with continuous action
\begin{quote}\begin{description}
\item[{Variables}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{history\_duration} (\sphinxstyleliteralemphasis{float}) -- Duration of the memory.

\item {} 
\sphinxstyleliteralstrong{simulation\_duration} (\sphinxstyleliteralemphasis{float}) -- Duration of the memory.

\item {} 
\sphinxstyleliteralstrong{size} (\sphinxstyleliteralemphasis{int}) -- size of the first dimension of the state.

\item {} 
\sphinxstyleliteralstrong{dt} (\sphinxstyleliteralemphasis{float}) -- time step between each value of the state.

\item {} 
\sphinxstyleliteralstrong{s} (\sphinxstyleliteralemphasis{np.array}\sphinxstyleliteralemphasis{(}\sphinxstyleliteralemphasis{)}) -- state containing the history of angles of attacks and velocities.

\item {} 
\sphinxstyleliteralstrong{idx\_memory} (\sphinxstyleliteralemphasis{range}) -- indices corresponding to the values shared by two successive states.

\item {} 
\sphinxstyleliteralstrong{simulator} ({\hyperref[\detokenize{package1:module-Simulator}]{\sphinxcrossref{\sphinxstyleliteralemphasis{Simulator}}}}) -- Simulator used to compute new values after a transition.

\item {} 
\sphinxstyleliteralstrong{reward} (\sphinxstyleliteralemphasis{float}) -- reward associated with a transition.

\item {} 
\sphinxstyleliteralstrong{discount} (\sphinxstyleliteralemphasis{float}) -- discount factor.

\item {} 
\sphinxstyleliteralstrong{action} (\sphinxstyleliteralemphasis{float}) -- action for transition.

\end{itemize}

\end{description}\end{quote}
\index{computeState() (mdp.ContinuousMDP method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:mdp.ContinuousMDP.computeState}}\pysiglinewithargsret{\sphinxbfcode{computeState}}{\emph{action}, \emph{WH}}{}
\end{fulllineitems}

\index{copy() (mdp.ContinuousMDP method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:mdp.ContinuousMDP.copy}}\pysiglinewithargsret{\sphinxbfcode{copy}}{}{}
\end{fulllineitems}

\index{extractSimulationData() (mdp.ContinuousMDP method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:mdp.ContinuousMDP.extractSimulationData}}\pysiglinewithargsret{\sphinxbfcode{extractSimulationData}}{}{}
\end{fulllineitems}

\index{initializeMDP() (mdp.ContinuousMDP method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:mdp.ContinuousMDP.initializeMDP}}\pysiglinewithargsret{\sphinxbfcode{initializeMDP}}{\emph{hdg0}, \emph{WH}}{}
\end{fulllineitems}

\index{initializeState() (mdp.ContinuousMDP method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:mdp.ContinuousMDP.initializeState}}\pysiglinewithargsret{\sphinxbfcode{initializeState}}{\emph{state}}{}
\end{fulllineitems}

\index{transition() (mdp.ContinuousMDP method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:mdp.ContinuousMDP.transition}}\pysiglinewithargsret{\sphinxbfcode{transition}}{\emph{action}, \emph{WH}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{MDP (class in mdp)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:mdp.MDP}}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{mdp.}\sphinxbfcode{MDP}}{\emph{duration\_history}, \emph{duration\_simulation}, \emph{delta\_t}}{}
Markov Decision process modelization of the transition
\begin{quote}\begin{description}
\item[{Variables}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{history\_duration} (\sphinxstyleliteralemphasis{float}) -- Duration of the memory.

\item {} 
\sphinxstyleliteralstrong{simulation\_duration} (\sphinxstyleliteralemphasis{float}) -- Duration of the memory.

\item {} 
\sphinxstyleliteralstrong{size} (\sphinxstyleliteralemphasis{int}) -- size of the first dimension of the state.

\item {} 
\sphinxstyleliteralstrong{dt} (\sphinxstyleliteralemphasis{float}) -- time step between each value of the state.

\item {} 
\sphinxstyleliteralstrong{s} (\sphinxstyleliteralemphasis{np.array}\sphinxstyleliteralemphasis{(}\sphinxstyleliteralemphasis{)}) -- state containing the history of angles of attacks and velocities.

\item {} 
\sphinxstyleliteralstrong{idx\_memory} (\sphinxstyleliteralemphasis{range}) -- indices corresponding to the values shared by two successive states.

\item {} 
\sphinxstyleliteralstrong{simulator} ({\hyperref[\detokenize{package1:module-Simulator}]{\sphinxcrossref{\sphinxstyleliteralemphasis{Simulator}}}}) -- Simulator used to compute new values after a transition.

\item {} 
\sphinxstyleliteralstrong{reward} (\sphinxstyleliteralemphasis{float}) -- reward associated with a transition.

\item {} 
\sphinxstyleliteralstrong{discount} (\sphinxstyleliteralemphasis{float}) -- discount factor.

\item {} 
\sphinxstyleliteralstrong{action} (\sphinxstyleliteralemphasis{float}) -- action for transition.

\end{itemize}

\end{description}\end{quote}
\index{computeState() (mdp.MDP method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:mdp.MDP.computeState}}\pysiglinewithargsret{\sphinxbfcode{computeState}}{\emph{action}, \emph{WH}}{}
Computes the mdp state when an action is applied.
:param action:
:param WH:
:return:

\end{fulllineitems}

\index{copy() (mdp.MDP method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:mdp.MDP.copy}}\pysiglinewithargsret{\sphinxbfcode{copy}}{}{}
Copy the MDP object
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
Deep copy of the object.

\item[{Return type}] \leavevmode
{\hyperref[\detokenize{package1:mdp.MDP}]{\sphinxcrossref{MDP}}}

\end{description}\end{quote}

\end{fulllineitems}

\index{extractSimulationData() (mdp.MDP method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:mdp.MDP.extractSimulationData}}\pysiglinewithargsret{\sphinxbfcode{extractSimulationData}}{}{}
\end{fulllineitems}

\index{initializeMDP() (mdp.MDP method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:mdp.MDP.initializeMDP}}\pysiglinewithargsret{\sphinxbfcode{initializeMDP}}{\emph{hdg0}, \emph{WH}}{}
Initialization of the Markov Decicison Process.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{hdg0} (\sphinxstyleliteralemphasis{float}) -- initial heading of the boat.

\item {} 
\sphinxstyleliteralstrong{np.array}\sphinxstyleliteralstrong{(}\sphinxstyleliteralstrong{)} (\sphinxstyleliteralemphasis{WH}) -- Vector of wind heading.

\end{itemize}

\item[{Returns}] \leavevmode
s initialized state

\item[{Return type}] \leavevmode
np.array()

\end{description}\end{quote}

\end{fulllineitems}

\index{policy() (mdp.MDP method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:mdp.MDP.policy}}\pysiglinewithargsret{\sphinxbfcode{policy}}{\emph{i\_treshold}}{}
\end{fulllineitems}

\index{transition() (mdp.MDP method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1:mdp.MDP.transition}}\pysiglinewithargsret{\sphinxbfcode{transition}}{\emph{action}, \emph{WH}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{action} -- action to make (either luff or bear off)

\item {} 
\sphinxstyleliteralstrong{WH} -- Wind heading provided by the environment during the transition.

\end{itemize}

\item[{Returns}] \leavevmode
The state and reward

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}


\begin{sphinxadmonition}{note}{Note:}
The class variable simulation\_duration defines the frequency of action taking. The reward is the average of the new velocities computed after each transition.
\end{sphinxadmonition}


\subsection{Tutorial}
\label{\detokenize{package1:tutorial}}
To visualize how a simulation can be generated we provide a file MDPmain.py that creates a simulation where the heading is first increase and then decrease.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{mdp}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k+kn}{as} \PYG{n+nn}{np}

\PYG{n}{TORAD} \PYG{o}{=} \PYG{n}{math}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{/} \PYG{l+m+mi}{180}

\PYG{n}{history\PYGZus{}duration} \PYG{o}{=} \PYG{l+m+mi}{3}
\PYG{n}{mdp\PYGZus{}step} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{time\PYGZus{}step} \PYG{o}{=} \PYG{l+m+mf}{0.1}
\PYG{n}{SP} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{40} \PYG{o}{*} \PYG{n}{TORAD}
\PYG{n}{mdp} \PYG{o}{=} \PYG{n}{mdp}\PYG{o}{.}\PYG{n}{MDP}\PYG{p}{(}\PYG{n}{history\PYGZus{}duration}\PYG{p}{,} \PYG{n}{mdp\PYGZus{}step}\PYG{p}{,} \PYG{n}{time\PYGZus{}step}\PYG{p}{)}

\PYG{n}{mean} \PYG{o}{=} \PYG{l+m+mi}{45} \PYG{o}{*} \PYG{n}{TORAD}
\PYG{n}{std} \PYG{o}{=} \PYG{l+m+mi}{0} \PYG{o}{*} \PYG{n}{TORAD}
\PYG{n}{wind\PYGZus{}samples} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{n}{WH} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{n}{mean} \PYG{o}{\PYGZhy{}} \PYG{n}{std}\PYG{p}{,} \PYG{n}{mean} \PYG{o}{+} \PYG{n}{std}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{n}{hdg0} \PYG{o}{=} \PYG{l+m+mi}{0} \PYG{o}{*} \PYG{n}{TORAD} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{wind\PYGZus{}samples}\PYG{p}{)}
\PYG{n}{state} \PYG{o}{=} \PYG{n}{mdp}\PYG{o}{.}\PYG{n}{initializeMDP}\PYG{p}{(}\PYG{n}{hdg0}\PYG{p}{,} \PYG{n}{WH}\PYG{p}{)}

\PYG{n}{SIMULATION\PYGZus{}TIME} \PYG{o}{=} \PYG{l+m+mi}{100}

\PYG{n}{i} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{vmg} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{wind\PYGZus{}heading} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{time} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{SIMULATION\PYGZus{}TIME}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t = \PYGZob{}0\PYGZcb{} s}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{time}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{action} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{n}{WH} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{n}{mean} \PYG{o}{\PYGZhy{}} \PYG{n}{std}\PYG{p}{,} \PYG{n}{mean} \PYG{o}{+} \PYG{n}{std}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{wind\PYGZus{}samples}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{time} \PYG{o}{\PYGZlt{}} \PYG{n}{SIMULATION\PYGZus{}TIME} \PYG{o}{/} \PYG{l+m+mi}{4}\PYG{p}{:}
        \PYG{n}{action} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{k}{elif} \PYG{n}{time} \PYG{o}{\PYGZlt{}} \PYG{n}{SIMULATION\PYGZus{}TIME} \PYG{o}{/} \PYG{l+m+mi}{2}\PYG{p}{:}
        \PYG{n}{action} \PYG{o}{=} \PYG{l+m+mi}{1}
    \PYG{k}{elif} \PYG{n}{time} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{3} \PYG{o}{*} \PYG{n}{SIMULATION\PYGZus{}TIME} \PYG{o}{/} \PYG{l+m+mi}{4}\PYG{p}{:}
        \PYG{n}{action} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{action} \PYG{o}{=} \PYG{l+m+mi}{1}

    \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward} \PYG{o}{=} \PYG{n}{mdp}\PYG{o}{.}\PYG{n}{transition}\PYG{p}{(}\PYG{n}{action}\PYG{p}{,} \PYG{n}{WH}\PYG{p}{)}
    \PYG{n}{next\PYGZus{}state} \PYG{o}{=} \PYG{n}{state}
    \PYG{n}{i} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{n}{mdp}\PYG{o}{.}\PYG{n}{extractSimulationData}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{vmg} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{[}\PYG{n}{vmg}\PYG{p}{,} \PYG{n}{mdp}\PYG{o}{.}\PYG{n}{extractSimulationData}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{wind\PYGZus{}heading} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{[}\PYG{n}{wind\PYGZus{}heading}\PYG{p}{,} \PYG{n}{WH}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{time\PYGZus{}vec} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{SIMULATION\PYGZus{}TIME}\PYG{p}{,} \PYG{n+nb}{int}\PYG{p}{(}\PYG{p}{(}\PYG{n}{SIMULATION\PYGZus{}TIME}\PYG{p}{)} \PYG{o}{/} \PYG{n}{time\PYGZus{}step}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{hdg} \PYG{o}{=} \PYG{n}{i} \PYG{o}{\PYGZhy{}} \PYG{n}{wind\PYGZus{}heading} \PYG{o}{\PYGZhy{}} \PYG{n}{SP}
\end{sphinxVerbatim}

This results in the following value for the velocity, angle of attack and heading.

\noindent{\hspace*{\fill}\scalebox{3.000000}{\sphinxincludegraphics[width=200\sphinxpxdimen,height=200\sphinxpxdimen]{{Figure_1}.png}}\hspace*{\fill}}


\section{Package Realistic Simulator}
\label{\detokenize{package1bis:package-realistic-simulator}}\label{\detokenize{package1bis::doc}}
This package contains all the classes required to build a realistic simulation of the boat. It uses the dynamic library \sphinxcode{libBoatModel.so} which implements the accurate dynamic of the boat. It is coded in C++ in order to speed up the calculation process and hence the learning.


\subsection{Realisitic Simulator}
\label{\detokenize{package1bis:module-Simulator_realistic}}\label{\detokenize{package1bis:realisitic-simulator}}\index{Simulator\_realistic (module)}\index{DW\_Model\_2\_EXPORT\_Discrete\_T (class in Simulator\_realistic)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.DW_Model_2_EXPORT_Discrete_T}}\pysigline{\sphinxstrong{class }\sphinxcode{Simulator\_realistic.}\sphinxbfcode{DW\_Model\_2\_EXPORT\_Discrete\_T}}
Bases: \sphinxcode{\_ctypes.Structure}
\index{DiscreteFilter\_states (Simulator\_realistic.DW\_Model\_2\_EXPORT\_Discrete\_T attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.DW_Model_2_EXPORT_Discrete_T.DiscreteFilter_states}}\pysigline{\sphinxbfcode{DiscreteFilter\_states}}
Structure/Union member

\end{fulllineitems}

\index{DiscreteTimeIntegrator1\_DSTATE (Simulator\_realistic.DW\_Model\_2\_EXPORT\_Discrete\_T attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.DW_Model_2_EXPORT_Discrete_T.DiscreteTimeIntegrator1_DSTATE}}\pysigline{\sphinxbfcode{DiscreteTimeIntegrator1\_DSTATE}}
Structure/Union member

\end{fulllineitems}

\index{DiscreteTimeIntegrator\_DSTATE (Simulator\_realistic.DW\_Model\_2\_EXPORT\_Discrete\_T attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.DW_Model_2_EXPORT_Discrete_T.DiscreteTimeIntegrator_DSTATE}}\pysigline{\sphinxbfcode{DiscreteTimeIntegrator\_DSTATE}}
Structure/Union member

\end{fulllineitems}

\index{DiscreteTimeIntegrator\_DSTATE\_p (Simulator\_realistic.DW\_Model\_2\_EXPORT\_Discrete\_T attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.DW_Model_2_EXPORT_Discrete_T.DiscreteTimeIntegrator_DSTATE_p}}\pysigline{\sphinxbfcode{DiscreteTimeIntegrator\_DSTATE\_p}}
Structure/Union member

\end{fulllineitems}

\index{DiscreteTimeIntegrator\_IC\_LOADI (Simulator\_realistic.DW\_Model\_2\_EXPORT\_Discrete\_T attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.DW_Model_2_EXPORT_Discrete_T.DiscreteTimeIntegrator_IC_LOADI}}\pysigline{\sphinxbfcode{DiscreteTimeIntegrator\_IC\_LOADI}}
Structure/Union member

\end{fulllineitems}

\index{DiscreteTimeIntegrator\_IC\_LOA\_m (Simulator\_realistic.DW\_Model\_2\_EXPORT\_Discrete\_T attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.DW_Model_2_EXPORT_Discrete_T.DiscreteTimeIntegrator_IC_LOA_m}}\pysigline{\sphinxbfcode{DiscreteTimeIntegrator\_IC\_LOA\_m}}
Structure/Union member

\end{fulllineitems}

\index{Discretemotormodel1\_states (Simulator\_realistic.DW\_Model\_2\_EXPORT\_Discrete\_T attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.DW_Model_2_EXPORT_Discrete_T.Discretemotormodel1_states}}\pysigline{\sphinxbfcode{Discretemotormodel1\_states}}
Structure/Union member

\end{fulllineitems}

\index{Discretemotormodel\_states (Simulator\_realistic.DW\_Model\_2\_EXPORT\_Discrete\_T attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.DW_Model_2_EXPORT_Discrete_T.Discretemotormodel_states}}\pysigline{\sphinxbfcode{Discretemotormodel\_states}}
Structure/Union member

\end{fulllineitems}

\index{FixPtUnitDelay1\_DSTATE (Simulator\_realistic.DW\_Model\_2\_EXPORT\_Discrete\_T attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.DW_Model_2_EXPORT_Discrete_T.FixPtUnitDelay1_DSTATE}}\pysigline{\sphinxbfcode{FixPtUnitDelay1\_DSTATE}}
Structure/Union member

\end{fulllineitems}

\index{FixPtUnitDelay2\_DSTATE (Simulator\_realistic.DW\_Model\_2\_EXPORT\_Discrete\_T attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.DW_Model_2_EXPORT_Discrete_T.FixPtUnitDelay2_DSTATE}}\pysigline{\sphinxbfcode{FixPtUnitDelay2\_DSTATE}}
Structure/Union member

\end{fulllineitems}

\index{G3\_PreviousInput (Simulator\_realistic.DW\_Model\_2\_EXPORT\_Discrete\_T attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.DW_Model_2_EXPORT_Discrete_T.G3_PreviousInput}}\pysigline{\sphinxbfcode{G3\_PreviousInput}}
Structure/Union member

\end{fulllineitems}

\index{PrevY (Simulator\_realistic.DW\_Model\_2\_EXPORT\_Discrete\_T attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.DW_Model_2_EXPORT_Discrete_T.PrevY}}\pysigline{\sphinxbfcode{PrevY}}
Structure/Union member

\end{fulllineitems}

\index{PrevY\_d (Simulator\_realistic.DW\_Model\_2\_EXPORT\_Discrete\_T attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.DW_Model_2_EXPORT_Discrete_T.PrevY_d}}\pysigline{\sphinxbfcode{PrevY\_d}}
Structure/Union member

\end{fulllineitems}

\index{incidencemeasure\_DSTATE (Simulator\_realistic.DW\_Model\_2\_EXPORT\_Discrete\_T attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.DW_Model_2_EXPORT_Discrete_T.incidencemeasure_DSTATE}}\pysigline{\sphinxbfcode{incidencemeasure\_DSTATE}}
Structure/Union member

\end{fulllineitems}

\index{speedmeasure\_DSTATE (Simulator\_realistic.DW\_Model\_2\_EXPORT\_Discrete\_T attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.DW_Model_2_EXPORT_Discrete_T.speedmeasure_DSTATE}}\pysigline{\sphinxbfcode{speedmeasure\_DSTATE}}
Structure/Union member

\end{fulllineitems}


\end{fulllineitems}

\index{RT\_MODEL\_Model\_2\_EXPORT\_Discr\_T (class in Simulator\_realistic)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.RT_MODEL_Model_2_EXPORT_Discr_T}}\pysigline{\sphinxstrong{class }\sphinxcode{Simulator\_realistic.}\sphinxbfcode{RT\_MODEL\_Model\_2\_EXPORT\_Discr\_T}}
Bases: \sphinxcode{\_ctypes.Structure}
\index{dwork (Simulator\_realistic.RT\_MODEL\_Model\_2\_EXPORT\_Discr\_T attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.RT_MODEL_Model_2_EXPORT_Discr_T.dwork}}\pysigline{\sphinxbfcode{dwork}}
Structure/Union member

\end{fulllineitems}


\end{fulllineitems}

\index{Simulator\_realistic (class in Simulator\_realistic)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.Simulator_realistic}}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{Simulator\_realistic.}\sphinxbfcode{Simulator\_realistic}}{\emph{simulation\_duration}, \emph{hdg0}, \emph{speed0}}{}
Bases: \sphinxcode{object}
\index{step() (Simulator\_realistic.Simulator\_realistic method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.Simulator_realistic.step}}\pysiglinewithargsret{\sphinxbfcode{step}}{\emph{delta\_hdg}, \emph{truewindheading}, \emph{truewindheading\_std}, \emph{duration}, \emph{precision}}{}
\end{fulllineitems}

\index{terminate() (Simulator\_realistic.Simulator\_realistic method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:Simulator_realistic.Simulator_realistic.terminate}}\pysiglinewithargsret{\sphinxbfcode{terminate}}{}{}
\end{fulllineitems}


\end{fulllineitems}


\begin{sphinxadmonition}{warning}{Warning:}
Be careful to use the libBoatModel library corresponding to your OS (Linux or Mac)
\end{sphinxadmonition}


\subsection{Realistic MDP}
\label{\detokenize{package1bis:module-mdp_realistic}}\label{\detokenize{package1bis:realistic-mdp}}\index{mdp\_realistic (module)}\index{mdp\_realistic (class in mdp\_realistic)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:mdp_realistic.mdp_realistic}}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{mdp\_realistic.}\sphinxbfcode{mdp\_realistic}}{\emph{mdp\_duration}, \emph{hdg0}, \emph{speed0}, \emph{history\_duration}, \emph{mdp\_step}, \emph{delta\_t}}{}
Bases: \sphinxcode{object}
\index{computeState() (mdp\_realistic.mdp\_realistic method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:mdp_realistic.mdp_realistic.computeState}}\pysiglinewithargsret{\sphinxbfcode{computeState}}{\emph{action}, \emph{truewindheading}, \emph{truewindheading\_std}}{}
Computes the mdp state when an action is applied.
:param action:
:param WH:
:return:

\end{fulllineitems}

\index{copy() (mdp\_realistic.mdp\_realistic method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:mdp_realistic.mdp_realistic.copy}}\pysiglinewithargsret{\sphinxbfcode{copy}}{}{}
Copy the MDP object
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
Deep copy of the object.

\item[{Return type}] \leavevmode
{\hyperref[\detokenize{package1:mdp.MDP}]{\sphinxcrossref{MDP}}}

\end{description}\end{quote}

\end{fulllineitems}

\index{extractSimulationData() (mdp\_realistic.mdp\_realistic method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:mdp_realistic.mdp_realistic.extractSimulationData}}\pysiglinewithargsret{\sphinxbfcode{extractSimulationData}}{}{}
\end{fulllineitems}

\index{initializeMDP() (mdp\_realistic.mdp\_realistic method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:mdp_realistic.mdp_realistic.initializeMDP}}\pysiglinewithargsret{\sphinxbfcode{initializeMDP}}{\emph{truewindheading}, \emph{truewindheading\_std}}{}
\end{fulllineitems}

\index{transition() (mdp\_realistic.mdp\_realistic method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package1bis:mdp_realistic.mdp_realistic.transition}}\pysiglinewithargsret{\sphinxbfcode{transition}}{\emph{action}, \emph{truewindheading}, \emph{truewindheading\_std}}{}
\end{fulllineitems}


\end{fulllineitems}



\section{Package RL}
\label{\detokenize{package2:package-rl}}\label{\detokenize{package2::doc}}

\subsection{Policy Learner}
\label{\detokenize{package2:policy-learner}}\label{\detokenize{package2:module-policyLearning}}\index{policyLearning (module)}\index{PolicyLearner (class in policyLearning)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:policyLearning.PolicyLearner}}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{policyLearning.}\sphinxbfcode{PolicyLearner}}{\emph{state\_size}, \emph{action\_size}, \emph{batch\_size}}{}
Bases: \sphinxcode{object}

The aim of this class is to learn the Q-value of the action defined by a policy.
\begin{quote}\begin{description}
\item[{Variables}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{state\_size} (\sphinxstyleliteralemphasis{int}) -- shape of the input (for convolutionnal layers).

\item {} 
\sphinxstyleliteralstrong{action\_size} (\sphinxstyleliteralemphasis{int}) -- number of action output by the network.

\item {} 
\sphinxstyleliteralstrong{memory} (\sphinxstyleliteralemphasis{deque}) -- last-in first-out list of the batch.

\item {} 
\sphinxstyleliteralstrong{gamma} (\sphinxstyleliteralemphasis{float}) -- discount factor.

\item {} 
\sphinxstyleliteralstrong{epsilon} (\sphinxstyleliteralemphasis{float}) -- exploration rate.

\item {} 
\sphinxstyleliteralstrong{epsilon\_min} (\sphinxstyleliteralemphasis{float}) -- smallest exploration rate that we want to converge to.

\item {} 
\sphinxstyleliteralstrong{epsilon\_decay} (\sphinxstyleliteralemphasis{float}) -- decay factor that we apply after each replay.

\item {} 
\sphinxstyleliteralstrong{learning\_rate} (\sphinxstyleliteralemphasis{float}) -- the learning rate of the NN.

\item {} 
\sphinxstyleliteralstrong{model} (\sphinxstyleliteralemphasis{keras.model}) -- NN, i.e the model containing the weight of the value estimator.

\end{itemize}

\end{description}\end{quote}
\index{act() (policyLearning.PolicyLearner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:policyLearning.PolicyLearner.act}}\pysiglinewithargsret{\sphinxbfcode{act}}{\emph{state}}{}
Calculate the action that yields the maximum Q-value.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{state} -- state in which we want to chose an action.

\item[{Returns}] \leavevmode
the greedy action.

\end{description}\end{quote}

\end{fulllineitems}

\index{actDeterministicallyUnderPolicy() (policyLearning.PolicyLearner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:policyLearning.PolicyLearner.actDeterministicallyUnderPolicy}}\pysiglinewithargsret{\sphinxbfcode{actDeterministicallyUnderPolicy}}{\emph{state}}{}
Policy that reattaches when the angle of attack goes higher than 16 degree
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{state} (\sphinxstyleliteralemphasis{np.array}) -- state for which we want to know the policy action.

\item[{Returns}] \leavevmode
the policy action.

\end{description}\end{quote}

\end{fulllineitems}

\index{actRandomly() (policyLearning.PolicyLearner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:policyLearning.PolicyLearner.actRandomly}}\pysiglinewithargsret{\sphinxbfcode{actRandomly}}{}{}
\end{fulllineitems}

\index{actUnderPolicy() (policyLearning.PolicyLearner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:policyLearning.PolicyLearner.actUnderPolicy}}\pysiglinewithargsret{\sphinxbfcode{actUnderPolicy}}{\emph{state}}{}
Does the same as {\hyperref[\detokenize{package2:policyLearning.PolicyLearner.actDeterministicallyUnderPolicy}]{\sphinxcrossref{\sphinxcode{actDeterministicallyUnderPolicy()}}}} instead that the returned action
is sometime taken randomly.

\end{fulllineitems}

\index{evaluate() (policyLearning.PolicyLearner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:policyLearning.PolicyLearner.evaluate}}\pysiglinewithargsret{\sphinxbfcode{evaluate}}{\emph{state}}{}
Evaluate the Q-value of the two actions in a given state using the neural network.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{state} (\sphinxstyleliteralemphasis{np.array}) -- state that we want to evaluate.

\item[{Returns}] \leavevmode
The actions values as a vector.

\end{description}\end{quote}

\end{fulllineitems}

\index{evaluateNextAction() (policyLearning.PolicyLearner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:policyLearning.PolicyLearner.evaluateNextAction}}\pysiglinewithargsret{\sphinxbfcode{evaluateNextAction}}{\emph{stall}}{}
Evaluate the next action without updating the stall state in order to use it during the experience replay
:param np.array state: state for which we want to know the policy action.
:return: the policy action.

\end{fulllineitems}

\index{get\_stall() (policyLearning.PolicyLearner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:policyLearning.PolicyLearner.get_stall}}\pysiglinewithargsret{\sphinxbfcode{get\_stall}}{}{}
\end{fulllineitems}

\index{init\_stall() (policyLearning.PolicyLearner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:policyLearning.PolicyLearner.init_stall}}\pysiglinewithargsret{\sphinxbfcode{init\_stall}}{\emph{mean}, \emph{mdp}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{mean} -- 

\item {} 
\sphinxstyleliteralstrong{mdp} -- 

\end{itemize}

\item[{Returns}] \leavevmode


\end{description}\end{quote}

\end{fulllineitems}

\index{load() (policyLearning.PolicyLearner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:policyLearning.PolicyLearner.load}}\pysiglinewithargsret{\sphinxbfcode{load}}{\emph{name}}{}
Load the weight of the network saved in the file into :ivar model
:param name: name of the file containing the weights to load

\end{fulllineitems}

\index{remember() (policyLearning.PolicyLearner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:policyLearning.PolicyLearner.remember}}\pysiglinewithargsret{\sphinxbfcode{remember}}{\emph{state}, \emph{action}, \emph{reward}, \emph{next\_state}, \emph{stall}}{}
Remember a transition defined by an action \sphinxtitleref{action} taken from a state \sphinxtitleref{state} yielding a transition to a next
state \sphinxtitleref{next\_state} and a reward \sphinxtitleref{reward}. {[}s, a ,r, s'{]}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{state} (\sphinxstyleliteralemphasis{np.array}) -- initial state (s).

\item {} 
\sphinxstyleliteralstrong{action} (\sphinxstyleliteralemphasis{int}) -- action (a).

\item {} 
\sphinxstyleliteralstrong{reward} (\sphinxstyleliteralemphasis{float}) -- reward received from transition (r).

\item {} 
\sphinxstyleliteralstrong{next\_state} (\sphinxstyleliteralemphasis{np.array}) -- final state (s').

\item {} 
\sphinxstyleliteralstrong{stall} (\sphinxstyleliteralemphasis{int}) -- flow state in the final state (s').

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{replay() (policyLearning.PolicyLearner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:policyLearning.PolicyLearner.replay}}\pysiglinewithargsret{\sphinxbfcode{replay}}{\emph{batch\_size}}{}
Perform the learning on a the experience replay memory.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{batch\_size} -- number of samples used in the experience replay memory for the fit.

\item[{Returns}] \leavevmode
the average loss over the replay batch.

\end{description}\end{quote}

\end{fulllineitems}

\index{save() (policyLearning.PolicyLearner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:policyLearning.PolicyLearner.save}}\pysiglinewithargsret{\sphinxbfcode{save}}{\emph{name}}{}
Save the weights of the newtork
:param name: Name of the file where the weights are saved

\end{fulllineitems}


\end{fulllineitems}



\subsection{DQN}
\label{\detokenize{package2:module-dqn}}\label{\detokenize{package2:dqn}}\index{dqn (module)}\index{DQNAgent (class in dqn)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:dqn.DQNAgent}}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{dqn.}\sphinxbfcode{DQNAgent}}{\emph{state\_size}, \emph{action\_size}}{}
Bases: \sphinxcode{object}

DQN agent
\begin{quote}\begin{description}
\item[{Variables}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{state\_size} (\sphinxstyleliteralemphasis{np.shape}\sphinxstyleliteralemphasis{(}\sphinxstyleliteralemphasis{)}) -- shape of the input.

\item {} 
\sphinxstyleliteralstrong{action\_size} (\sphinxstyleliteralemphasis{int}) -- number of actions.

\item {} 
\sphinxstyleliteralstrong{memory} (\sphinxstyleliteralemphasis{deque}\sphinxstyleliteralemphasis{(}\sphinxstyleliteralemphasis{)}) -- memory as a list.

\item {} 
\sphinxstyleliteralstrong{gamma} (\sphinxstyleliteralemphasis{float}) -- Discount rate.

\item {} 
\sphinxstyleliteralstrong{epsilon} (\sphinxstyleliteralemphasis{float}) -- exploration rate.

\item {} 
\sphinxstyleliteralstrong{epsilon\_min} (\sphinxstyleliteralemphasis{float}) -- minimum exploration rate.

\item {} 
\sphinxstyleliteralstrong{epsilon\_decay} (\sphinxstyleliteralemphasis{float}) -- decay of the exploration rate.

\item {} 
\sphinxstyleliteralstrong{learning\_rate} (\sphinxstyleliteralemphasis{float}) -- initial learning rate for the gradient descent

\item {} 
\sphinxstyleliteralstrong{model} (\sphinxstyleliteralemphasis{keras.model}) -- neural network model

\end{itemize}

\end{description}\end{quote}
\index{act() (dqn.DQNAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:dqn.DQNAgent.act}}\pysiglinewithargsret{\sphinxbfcode{act}}{\emph{state}}{}
Act \(\epsilon\)-greedy with respect to the actual Q-value output by the network.
:param state: State from which we want to use the network to compute the action to take.
:return: a random action with probability \(\epsilon\) or the greedy action with probability 1-\(\epsilon\).

\end{fulllineitems}

\index{actDeterministically() (dqn.DQNAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:dqn.DQNAgent.actDeterministically}}\pysiglinewithargsret{\sphinxbfcode{actDeterministically}}{\emph{state}}{}
Predicts the action with the highest q-value at a given state.
:param state: state from which we want to know the action to make.
:return:

\end{fulllineitems}

\index{load() (dqn.DQNAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:dqn.DQNAgent.load}}\pysiglinewithargsret{\sphinxbfcode{load}}{\emph{name}}{}
Load the weights for a defined architecture.
:param name: Name of the source file.

\end{fulllineitems}

\index{loadModel() (dqn.DQNAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:dqn.DQNAgent.loadModel}}\pysiglinewithargsret{\sphinxbfcode{loadModel}}{\emph{name}}{}
Load the an architecture from source file.
:param name: Name of the source file.
:return:

\end{fulllineitems}

\index{remember() (dqn.DQNAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:dqn.DQNAgent.remember}}\pysiglinewithargsret{\sphinxbfcode{remember}}{\emph{state}, \emph{action}, \emph{reward}, \emph{next\_state}}{}
Remember a transition defined by an action \sphinxtitleref{action} taken from a state \sphinxtitleref{state} yielding a transition to a next
state \sphinxtitleref{next\_state} and a reward \sphinxtitleref{reward}. {[}s, a ,r, s'{]}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{state} (\sphinxstyleliteralemphasis{np.array}) -- initial state (s).

\item {} 
\sphinxstyleliteralstrong{action} (\sphinxstyleliteralemphasis{int}) -- action (a).

\item {} 
\sphinxstyleliteralstrong{reward} (\sphinxstyleliteralemphasis{float}) -- reward received from transition (r).

\item {} 
\sphinxstyleliteralstrong{next\_state} (\sphinxstyleliteralemphasis{np.array}) -- final state (s').

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{replay() (dqn.DQNAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:dqn.DQNAgent.replay}}\pysiglinewithargsret{\sphinxbfcode{replay}}{\emph{batch\_size}}{}
Core of the algorithm Q update according to the current weight of the network.
:param int batch\_size: Batch size for the batch gradient descent.
:return: the loss after the batch gradient descent.

\end{fulllineitems}

\index{save() (dqn.DQNAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:dqn.DQNAgent.save}}\pysiglinewithargsret{\sphinxbfcode{save}}{\emph{name}}{}
Save the weights for a defined architecture.
:param name: Name of the output file

\end{fulllineitems}

\index{saveModel() (dqn.DQNAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:dqn.DQNAgent.saveModel}}\pysiglinewithargsret{\sphinxbfcode{saveModel}}{\emph{name}}{}
Save the model's weight and architecture.
:param name: Name of the output file

\end{fulllineitems}


\end{fulllineitems}



\subsection{DDPG}
\label{\detokenize{package2:ddpg}}\label{\detokenize{package2:module-DDPG}}\index{DDPG (module)}\index{DDPGAgent (class in DDPG)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:DDPG.DDPGAgent}}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{DDPG.}\sphinxbfcode{DDPGAgent}}{\emph{state\_size}, \emph{action\_size}, \emph{lower\_bound}, \emph{upper\_bound}, \emph{sess}}{}
Bases: \sphinxcode{object}

The aim of this class is to learn an optimal policy via an actor-critic structure with 2 separated Convolutional Neural Networks
It uses the Deep Deterministic Policy Gradient to update tha actor network.
This model deals with a continuous space of actions on the rudder, chosen between lower\_bound and upper\_bound.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{state\_size} (\sphinxstyleliteralemphasis{int}) -- length of the state input (for convolutionnal layers).

\item {} 
\sphinxstyleliteralstrong{action\_size} (\sphinxstyleliteralemphasis{int}) -- number of continuous action output by the network.

\item {} 
\sphinxstyleliteralstrong{lower\_bound} (\sphinxstyleliteralemphasis{float}) -- minimum value for rudder action.

\item {} 
\sphinxstyleliteralstrong{upper\_bound} (\sphinxstyleliteralemphasis{float}) -- maximum value for rudder action.

\item {} 
\sphinxstyleliteralstrong{sess} (\sphinxstyleliteralemphasis{tensorflow.session}) -- initialized tensorflow session within which the agent will be trained.

\end{itemize}

\item[{Variables}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{memory} (\sphinxstyleliteralemphasis{deque}) -- last-in first-out list of the batch buffer.

\item {} 
\sphinxstyleliteralstrong{gamma} (\sphinxstyleliteralemphasis{float}) -- discount factor.

\item {} 
\sphinxstyleliteralstrong{epsilon} (\sphinxstyleliteralemphasis{float}) -- exploration rate.

\item {} 
\sphinxstyleliteralstrong{epsilon\_min} (\sphinxstyleliteralemphasis{float}) -- smallest exploration rate that we want to converge to.

\item {} 
\sphinxstyleliteralstrong{epsilon\_decay} (\sphinxstyleliteralemphasis{float}) -- decay factor that we apply after each replay.

\item {} 
\sphinxstyleliteralstrong{actor\_learning\_rate} (\sphinxstyleliteralemphasis{float}) -- the learning rate of the NN of actor.

\item {} 
\sphinxstyleliteralstrong{critic\_learning\_rate} (\sphinxstyleliteralemphasis{float}) -- the learning rate of the NN of critic.

\item {} 
{\hyperref[\detokenize{package2:DDPG.DDPGAgent.update_target}]{\sphinxcrossref{\sphinxstyleliteralstrong{update\_target}}}} (\sphinxstyleliteralemphasis{float}) -- update factor of the Neural Networks for each fit to target

\item {} 
\sphinxstyleliteralstrong{network} (\sphinxstyleliteralemphasis{DDPGNetworks.Network}) -- tensorflow model which defines actor and critic convolutional neural networks features

\end{itemize}

\end{description}\end{quote}
\index{act() (DDPG.DDPGAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:DDPG.DDPGAgent.act}}\pysiglinewithargsret{\sphinxbfcode{act}}{\emph{state}}{}
Calculate the action given by the Actor network's current weights
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{state} -- state in which we want to chose an action.

\item[{Returns}] \leavevmode
the greedy action according to actor network

\end{description}\end{quote}

\end{fulllineitems}

\index{act\_epsilon\_greedy() (DDPG.DDPGAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:DDPG.DDPGAgent.act_epsilon_greedy}}\pysiglinewithargsret{\sphinxbfcode{act\_epsilon\_greedy}}{\emph{state}}{}
With probability epsilon, returns a random action between bounds
With probability 1 - epsilon, returns the action given by the Actor network's current weights
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{state} -- state in which we want to chose an action.

\item[{Returns}] \leavevmode
a random action or the action given by actor

\end{description}\end{quote}

\end{fulllineitems}

\index{evaluate() (DDPG.DDPGAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:DDPG.DDPGAgent.evaluate}}\pysiglinewithargsret{\sphinxbfcode{evaluate}}{\emph{state}, \emph{action}}{}
Evaluate the Q-value of a state-action pair  using the critic neural network.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{state} (\sphinxstyleliteralemphasis{np.array}) -- state that we want to evaluate.

\item {} 
\sphinxstyleliteralstrong{action} (\sphinxstyleliteralemphasis{float}) -- action that we want to evaluate (has to be between permitted bounds)

\end{itemize}

\item[{Returns}] \leavevmode
The continuous action value.

\end{description}\end{quote}

\end{fulllineitems}

\index{load() (DDPG.DDPGAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:DDPG.DDPGAgent.load}}\pysiglinewithargsret{\sphinxbfcode{load}}{\emph{name}}{}
Load the weights of the 2 networks saved in the file into :ivar network
:param name: name of the file containing the weights to load

\end{fulllineitems}

\index{noise\_decay() (DDPG.DDPGAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:DDPG.DDPGAgent.noise_decay}}\pysiglinewithargsret{\sphinxbfcode{noise\_decay}}{\emph{e}}{}
Applies decay on noisy epsilon-greedy actions
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{e} -- current episode playing during learning

\end{description}\end{quote}

\end{fulllineitems}

\index{remember() (DDPG.DDPGAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:DDPG.DDPGAgent.remember}}\pysiglinewithargsret{\sphinxbfcode{remember}}{\emph{state}, \emph{action}, \emph{reward}, \emph{next\_state}}{}
Remember a transition defined by an action \sphinxtitleref{action} taken from a state \sphinxtitleref{state} yielding a transition to a next
state \sphinxtitleref{next\_state} and a reward \sphinxtitleref{reward}. {[}s, a ,r, s'{]}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{state} (\sphinxstyleliteralemphasis{np.array}) -- initial state (s).

\item {} 
\sphinxstyleliteralstrong{action} (\sphinxstyleliteralemphasis{int}) -- action (a).

\item {} 
\sphinxstyleliteralstrong{reward} (\sphinxstyleliteralemphasis{float}) -- reward received from transition (r).

\item {} 
\sphinxstyleliteralstrong{next\_state} (\sphinxstyleliteralemphasis{np.array}) -- final state (s').

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{replay() (DDPG.DDPGAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:DDPG.DDPGAgent.replay}}\pysiglinewithargsret{\sphinxbfcode{replay}}{\emph{batch\_size}}{}
Performs an update of both actor and critic networks on a minibatch chosen among the experience replay memory.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{batch\_size} -- number of samples used in the experience replay memory for the fit.

\item[{Returns}] \leavevmode
the average losses for actor and critic over the replay batch.

\end{description}\end{quote}

\end{fulllineitems}

\index{save() (DDPG.DDPGAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:DDPG.DDPGAgent.save}}\pysiglinewithargsret{\sphinxbfcode{save}}{\emph{name}}{}
Save the weights of both of the networks into a .ckpt tensorflow session file
:param name: Name of the file where the weights are saved

\end{fulllineitems}

\index{update\_target (DDPG.DDPGAgent attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{package2:DDPG.DDPGAgent.update_target}}\pysigline{\sphinxbfcode{update\_target}\sphinxstrong{ = None}}
Definition of the neural networks

\end{fulllineitems}


\end{fulllineitems}



\subsection{Tutorial}
\label{\detokenize{package2:tutorial}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
 \PYG{n}{history\PYGZus{}duration} \PYG{o}{=} \PYG{l+m+mi}{3}  \PYG{c+c1}{\PYGZsh{} Duration of state history [s]}
 \PYG{n}{mdp\PYGZus{}step} \PYG{o}{=} \PYG{l+m+mi}{1}  \PYG{c+c1}{\PYGZsh{} Step between each state transition [s]}
 \PYG{n}{time\PYGZus{}step} \PYG{o}{=} \PYG{l+m+mf}{0.1}  \PYG{c+c1}{\PYGZsh{} time step [s] \PYGZlt{}\PYGZhy{}\PYGZgt{} 10Hz frequency of data acquisition}
 \PYG{n}{mdp} \PYG{o}{=} \PYG{n}{MDP}\PYG{p}{(}\PYG{n}{history\PYGZus{}duration}\PYG{p}{,} \PYG{n}{mdp\PYGZus{}step}\PYG{p}{,} \PYG{n}{time\PYGZus{}step}\PYG{p}{)}

 \PYG{n}{mean} \PYG{o}{=} \PYG{l+m+mi}{45} \PYG{o}{*} \PYG{n}{TORAD}
 \PYG{n}{std} \PYG{o}{=} \PYG{l+m+mi}{0} \PYG{o}{*} \PYG{n}{TORAD}
 \PYG{n}{wind\PYGZus{}samples} \PYG{o}{=} \PYG{l+m+mi}{10}
 \PYG{n}{WH} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{n}{mean} \PYG{o}{\PYGZhy{}} \PYG{n}{std}\PYG{p}{,} \PYG{n}{mean} \PYG{o}{+} \PYG{n}{std}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

 \PYG{n}{hdg0}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}
 \PYG{n}{mdp}\PYG{o}{.}\PYG{n}{initializeMDP}\PYG{p}{(}\PYG{n}{hdg0}\PYG{p}{,}\PYG{n}{WH}\PYG{p}{)}

 \PYG{n}{hdg0\PYGZus{}rand\PYGZus{}vec}\PYG{o}{=}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+m+mi}{8}\PYG{p}{,}\PYG{l+m+mi}{18}\PYG{p}{,}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{21}\PYG{p}{,}\PYG{l+m+mi}{22}\PYG{p}{,}\PYG{l+m+mi}{24}\PYG{p}{)}

 \PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{2}
 \PYG{n}{policy\PYGZus{}angle} \PYG{o}{=} \PYG{l+m+mi}{18}
 \PYG{n}{agent} \PYG{o}{=} \PYG{n}{PolicyLearner}\PYG{p}{(}\PYG{n}{mdp}\PYG{o}{.}\PYG{n}{size}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,} \PYG{n}{policy\PYGZus{}angle}\PYG{p}{)}
 \PYG{c+c1}{\PYGZsh{}agent.load(\PYGZdq{}policy\PYGZus{}learning\PYGZus{}i18\PYGZus{}test\PYGZus{}long\PYGZus{}history\PYGZdq{})}
 \PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{120}

 \PYG{n}{EPISODES} \PYG{o}{=} \PYG{l+m+mi}{500}

 \PYG{k}{for} \PYG{n}{e} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{EPISODES}\PYG{p}{)}\PYG{p}{:}
     \PYG{n}{WH} \PYG{o}{=} \PYG{n}{w}\PYG{o}{.}\PYG{n}{generateWind}\PYG{p}{(}\PYG{p}{)}
     \PYG{n}{hdg0\PYGZus{}rand} \PYG{o}{=} \PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{hdg0\PYGZus{}rand\PYGZus{}vec}\PYG{p}{)} \PYG{o}{*} \PYG{n}{TORAD}
     \PYG{n}{hdg0} \PYG{o}{=} \PYG{n}{hdg0\PYGZus{}rand} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}

     \PYG{n}{mdp}\PYG{o}{.}\PYG{n}{simulator}\PYG{o}{.}\PYG{n}{hyst}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}

     \PYG{c+c1}{\PYGZsh{}  We reinitialize the memory of the flow}
     \PYG{n}{state} \PYG{o}{=} \PYG{n}{mdp}\PYG{o}{.}\PYG{n}{initializeMDP}\PYG{p}{(}\PYG{n}{hdg0}\PYG{p}{,} \PYG{n}{WH}\PYG{p}{)}
     \PYG{n}{loss\PYGZus{}sim\PYGZus{}list} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
     \PYG{k}{for} \PYG{n}{time} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{80}\PYG{p}{)}\PYG{p}{:}
         \PYG{n}{WH} \PYG{o}{=} \PYG{n}{w}\PYG{o}{.}\PYG{n}{generateWind}\PYG{p}{(}\PYG{p}{)}
         \PYG{n}{action} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{act}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
         \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward} \PYG{o}{=} \PYG{n}{mdp}\PYG{o}{.}\PYG{n}{transition}\PYG{p}{(}\PYG{n}{action}\PYG{p}{,} \PYG{n}{WH}\PYG{p}{)}
         \PYG{n}{agent}\PYG{o}{.}\PYG{n}{remember}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} store the transition + the state flow in the}
         \PYG{c+c1}{\PYGZsh{} final state !!}
         \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
         \PYG{k}{if} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{agent}\PYG{o}{.}\PYG{n}{memory}\PYG{p}{)} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n}{batch\PYGZus{}size}\PYG{p}{:}
             \PYG{n}{loss\PYGZus{}sim\PYGZus{}list}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{agent}\PYG{o}{.}\PYG{n}{replay}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{p}{)}\PYG{p}{)}
             \PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{time: \PYGZob{}\PYGZcb{}, Loss = \PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{time}\PYG{p}{,} \PYG{n}{loss\PYGZus{}sim\PYGZus{}list}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
             \PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{i : \PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{mdp}\PYG{o}{.}\PYG{n}{s}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{/} \PYG{n}{TORAD}\PYG{p}{)}\PYG{p}{)}
         \PYG{c+c1}{\PYGZsh{} For data visualisation}
     \PYG{n}{loss\PYGZus{}over\PYGZus{}simulation\PYGZus{}time} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{los}    \PYG{n}{s\PYGZus{}sim\PYGZus{}list}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{loss\PYGZus{}sim\PYGZus{}list}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
     \PYG{n}{loss\PYGZus{}of\PYGZus{}episode}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{loss\PYGZus{}over\PYGZus{}simulation\PYGZus{}time}\PYG{p}{)}
\end{sphinxVerbatim}


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{d}
\item {\sphinxstyleindexentry{DDPG}}\sphinxstyleindexpageref{package2:\detokenize{module-DDPG}}
\item {\sphinxstyleindexentry{dqn}}\sphinxstyleindexpageref{package2:\detokenize{module-dqn}}
\indexspace
\bigletter{e}
\item {\sphinxstyleindexentry{environment}}\sphinxstyleindexpageref{package1:\detokenize{module-environment}}
\indexspace
\bigletter{h}
\item {\sphinxstyleindexentry{Hysteresis}}\sphinxstyleindexpageref{package1:\detokenize{module-Hysteresis}}
\indexspace
\bigletter{m}
\item {\sphinxstyleindexentry{mdp}}\sphinxstyleindexpageref{package1:\detokenize{module-mdp}}
\item {\sphinxstyleindexentry{mdp\_realistic}}\sphinxstyleindexpageref{package1bis:\detokenize{module-mdp_realistic}}
\indexspace
\bigletter{p}
\item {\sphinxstyleindexentry{policyLearning}}\sphinxstyleindexpageref{package2:\detokenize{module-policyLearning}}
\indexspace
\bigletter{s}
\item {\sphinxstyleindexentry{Simulator}}\sphinxstyleindexpageref{package1:\detokenize{module-Simulator}}
\item {\sphinxstyleindexentry{Simulator\_realistic}}\sphinxstyleindexpageref{package1bis:\detokenize{module-Simulator_realistic}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}